# -*- coding: utf-8 -*-
"""–ê–ª–≥–æ—Å—ã

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFZThCa2yg0Rhe0nSa7jceRy3mAaiHr3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò
# ============================================================================

class NeuralNetworkConfig:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏."""

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    TEST_SIZE: float = 0.2
    VALIDATION_SPLIT: float = 0.2
    RANDOM_STATE: int = 42

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    FEATURES_TO_USE: list = [
        'alpha', 'delta', 'u', 'g', 'r', 'i', 'z',
        'run_ID', 'cam_col', 'field_ID',
        'redshift', 'plate', 'MJD', 'fiber_ID'
    ]
    # –í–ê–ñ–ù–û: —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ —Ä–∞–≤–µ–Ω —á–∏—Å–ª—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    INPUT_DIM: int = len(FEATURES_TO_USE)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    HIDDEN_LAYERS: list = [256, 128, 64, 32]
    OUTPUT_DIM: int = 3
    ACTIVATION_HIDDEN: str = 'relu'
    ACTIVATION_OUTPUT: str = 'softmax'
    DROPOUT_RATE: float = 0.3

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    EPOCHS: int = 50
    BATCH_SIZE: int = 128
    LEARNING_RATE: float = 0.001
    OPTIMIZER: str = 'adam'

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    EARLY_STOPPING_PATIENCE: int = 10
    EARLY_STOPPING_MONITOR: str = 'val_loss'
    EARLY_STOPPING_RESTORE_WEIGHTS: bool = True

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    ANOMALY_VALUE: float = -9999.0
    REPLACE_WITH_MEAN: bool = True


# ============================================================================
# –ö–õ–ê–°–° –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –ò –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–ù–ù–´–•
# ============================================================================

class DataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""

    def __init__(self, config: NeuralNetworkConfig) -> None:
        self.config = config
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None

    def load_data(self, file_path: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ CSV —Ñ–∞–π–ª–∞."""
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {file_path}")
        df = pd.read_csv(file_path)
        print(
            f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, "
            f"{df.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
        )
        return df

    def handle_anomalies(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (-9999)."""
        print("üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            anomaly_count = (df[col] == self.config.ANOMALY_VALUE).sum()
            if anomaly_count > 0 and self.config.REPLACE_WITH_MEAN:
                mean_val = df[df[col] != self.config.ANOMALY_VALUE][col].mean()
                df[col] = df[col].replace(self.config.ANOMALY_VALUE, mean_val)
                print(
                    f"  ‚Üí {col}: {anomaly_count} –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ"
                )

        return df

    def prepare_features_and_labels(
        self,
        df: pd.DataFrame
    ) -> tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–∞."""
        print("‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        X = df[self.config.FEATURES_TO_USE].values
        y = self.label_encoder.fit_transform(df['class'])
        y_categorical = keras.utils.to_categorical(
            y,
            num_classes=self.config.OUTPUT_DIM
        )

        print(f"  ‚úì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")
        print(f"  ‚úì –ö–ª–∞—Å—Å—ã: {self.label_encoder.classes_}")
        print("  ‚úì –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        for idx, class_name in enumerate(self.label_encoder.classes_):
            count = (y == idx).sum()
            percentage = (count / len(y)) * 100
            print(f"    - {class_name}: {count} ({percentage:.1f}%)")

        return X, y_categorical

    def split_and_normalize(
        self,
        X: np.ndarray,
        y: np.ndarray
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–µ/–æ–±—É—á–∞—é—â–µ–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è."""
        print("üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=self.config.TEST_SIZE,
            random_state=self.config.RANDOM_STATE,
            stratify=y
        )

        X_train_norm = self.scaler.fit_transform(X_train)
        X_test_norm = self.scaler.transform(X_test)

        print(f"  ‚úì –û–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä: {X_train_norm.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"  ‚úì –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {X_test_norm.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

        self.X_train = X_train_norm
        self.X_test = X_test_norm
        self.y_train = y_train
        self.y_test = y_test

        return X_train_norm, X_test_norm, y_train, y_test


# ============================================================================
# –ö–õ–ê–°–° –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò
# ============================================================================

class AstronomyNeuralNetwork:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏."""

    def __init__(self, config: NeuralNetworkConfig) -> None:
        self.config = config
        self.model: keras.Model | None = None
        self.history: keras.callbacks.History | None = None

    def build_model(self) -> keras.Model:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏."""
        print("üèóÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")

        model = keras.Sequential()
        model.add(
            layers.Input(
                shape=(self.config.INPUT_DIM,),
                name='input_layer'
            )
        )

        for i, units in enumerate(self.config.HIDDEN_LAYERS):
            model.add(
                layers.Dense(
                    units,
                    activation=self.config.ACTIVATION_HIDDEN,
                    kernel_initializer='he_normal',
                    name=f'hidden_layer_{i + 1}'
                )
            )
            model.add(
                layers.Dropout(
                    self.config.DROPOUT_RATE,
                    name=f'dropout_{i + 1}'
                )
            )
            print(f"  ‚úì –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π {i + 1}: {units} –Ω–µ–π—Ä–æ–Ω–æ–≤")

        model.add(
            layers.Dense(
                self.config.OUTPUT_DIM,
                activation=self.config.ACTIVATION_OUTPUT,
                name='output_layer'
            )
        )
        print(f"  ‚úì –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: {self.config.OUTPUT_DIM} –Ω–µ–π—Ä–æ–Ω–æ–≤")

        optimizer = Adam(learning_rate=self.config.LEARNING_RATE)
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        self.model = model
        return model

    def train_model(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray | None = None,
        y_val: np.ndarray | None = None
    ) -> keras.callbacks.History:
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏."""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        print(f"  –≠–ø–æ—Ö–∏: {self.config.EPOCHS}")
        print(f"  –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {self.config.BATCH_SIZE}")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {self.config.LEARNING_RATE}")

        early_stopping = EarlyStopping(
            monitor=self.config.EARLY_STOPPING_MONITOR,
            patience=self.config.EARLY_STOPPING_PATIENCE,
            restore_best_weights=self.config.EARLY_STOPPING_RESTORE_WEIGHTS,
            verbose=1
        )

        validation_data = None
        if X_val is not None and y_val is not None:
            validation_data = (X_val, y_val)

        history = self.model.fit(
            X_train,
            y_train,
            epochs=self.config.EPOCHS,
            batch_size=self.config.BATCH_SIZE,
            validation_data=validation_data,
            callbacks=[early_stopping],
            verbose=1
        )

        self.history = history
        print("‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return history

    def evaluate_model(
        self,
        X_test: np.ndarray,
        y_test: np.ndarray
    ) -> dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ."""
        print("\nüìà –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")

        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        print(f"  ‚úì –ü–æ—Ç–µ—Ä–∏: {loss:.4f}")
        print(f"  ‚úì –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f} ({accuracy * 100:.2f}%)")

        return {'loss': float(loss), 'accuracy': float(accuracy)}

    def get_predictions(self, X: np.ndarray) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏."""
        return self.model.predict(X, verbose=0)


# ============================================================================
# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
# ============================================================================

class Visualizer:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫."""

    @staticmethod
    def plot_training_history(
        history: keras.callbacks.History
    ) -> None:
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è."""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))

        axes[0].plot(
            history.history['accuracy'],
            'b-',
            linewidth=2,
            label='–û–±—É—á–µ–Ω–∏–µ'
        )
        axes[0].plot(
            history.history['val_accuracy'],
            'r-',
            linewidth=2,
            label='–í–∞–ª–∏–¥–∞—Ü–∏—è'
        )
        axes[0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        axes[1].plot(
            history.history['loss'],
            'b-',
            linewidth=2,
            label='–û–±—É—á–µ–Ω–∏–µ'
        )
        axes[1].plot(
            history.history['val_loss'],
            'r-',
            linewidth=2,
            label='–í–∞–ª–∏–¥–∞—Ü–∏—è'
        )
        axes[1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
        axes[1].set_title('–ü–æ—Ç–µ—Ä–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=100, bbox_inches='tight')
        plt.show()
        print("‚úì –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")

    @staticmethod
    def plot_confusion_matrix(
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: list[str]
    ) -> None:
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫."""
        y_true_classes = np.argmax(y_true, axis=1)
        y_pred_classes = np.argmax(y_pred, axis=1)

        cm = confusion_matrix(y_true_classes, y_pred_classes)

        plt.figure(figsize=(8, 6))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names
        )
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=100, bbox_inches='tight')
        plt.show()
        print("‚úì –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

    @staticmethod
    def plot_class_distribution(
        y: np.ndarray,
        class_names: list[str]
    ) -> None:
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤."""
        y_classes = np.argmax(y, axis=1)
        unique, counts = np.unique(y_classes, return_counts=True)

        plt.figure(figsize=(8, 5))
        bars = plt.bar(
            [class_names[i] for i in unique],
            counts,
            color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
            edgecolor='black'
        )

        for bar in bars:
            height = bar.get_height()
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                height,
                f'{int(height)}',
                ha='center',
                va='bottom'
            )

        plt.xlabel('–ö–ª–∞—Å—Å')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤')
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig('class_distribution.png', dpi=100, bbox_inches='tight')
        plt.show()
        print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")


# ============================================================================
# –ì–õ–ê–í–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞."""
    print("=" * 75)
    print("–ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ê–°–¢–†–û–ù–û–ú–ò–ß–ï–°–ö–ò–• –û–ë–™–ï–ö–¢–û–í –° –ü–û–ú–û–©–¨–Æ –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
    print("–ê–≤—Ç–æ—Ä—ã: –†—ã–∂–∫–æ–≤ –ê—Ä—Ç—ë–º, –ì—Ä–∏—Ü–∫–æ–≤ –í–∞—Å–∏–ª–∏–π")
    print("=" * 75)

    config = NeuralNetworkConfig()
    preprocessor = DataPreprocessor(config)

    print("\n" + "=" * 75)
    print("–≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
    print("=" * 75)

    df = preprocessor.load_data('star_classification-3.csv')
    df = preprocessor.handle_anomalies(df)
    X, y = preprocessor.prepare_features_and_labels(df)
    X_train, X_test, y_train, y_test = preprocessor.split_and_normalize(X, y)

    X_train, X_val, y_train, y_val = train_test_split(
        X_train,
        y_train,
        test_size=config.VALIDATION_SPLIT,
        random_state=config.RANDOM_STATE,
        stratify=y_train
    )
    print(f"‚úì –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä: {X_val.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

    print("\n" + "=" * 75)
    print("–≠–¢–ê–ü 2: –ü–û–°–¢–†–û–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
    print("=" * 75)

    nn = AstronomyNeuralNetwork(config)
    model = nn.build_model()
    print("\nüìã –†–µ–∑—é–º–µ –º–æ–¥–µ–ª–∏:")
    model.summary()

    print("\n" + "=" * 75)
    print("–≠–¢–ê–ü 3: –û–ë–£–ß–ï–ù–ò–ï –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
    print("=" * 75)

    history = nn.train_model(X_train, y_train, X_val, y_val)

    print("\n" + "=" * 75)
    print("–≠–¢–ê–ü 4: –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
    print("=" * 75)

    test_metrics = nn.evaluate_model(X_test, y_test)
    y_pred = nn.get_predictions(X_test)

    y_true_classes = np.argmax(y_test, axis=1)
    y_pred_classes = np.argmax(y_pred, axis=1)

    print("\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    print(
        classification_report(
            y_true_classes,
            y_pred_classes,
            target_names=preprocessor.label_encoder.classes_
        )
    )

    print("\n" + "=" * 75)
    print("–≠–¢–ê–ü 5: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 75)

    visualizer = Visualizer()
    visualizer.plot_training_history(history)
    visualizer.plot_confusion_matrix(
        y_test,
        y_pred,
        preprocessor.label_encoder.classes_
    )
    visualizer.plot_class_distribution(
        y,
        preprocessor.label_encoder.classes_
    )

    print("\n" + "=" * 75)
    print("–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 75)
    print(
        f"‚úì –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ: "
        f"{test_metrics['accuracy'] * 100:.2f}%"
    )
    print(f"‚úì –ò—Ç–æ–≥–æ–≤—ã–µ –ø–æ—Ç–µ—Ä–∏: {test_metrics['loss']:.4f}")
    print(
        f"‚úì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: "
        f"{len(history.history['loss'])}"
    )
    print("\n‚úì –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é!")
    print("=" * 75)

    return nn, preprocessor, history, test_metrics


if __name__ == "__main__":
    nn, preprocessor, history, metrics = main()
